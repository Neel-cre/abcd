# -*- coding: utf-8 -*-
"""Assignment_EE798R_Ayush_210245_improvement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KLPlugaQQFMZUSbLw4cPBeZ1KzZdqFqB
"""

import csv
import glob
import nltk
import numpy as np
from sklearn.preprocessing import OrdinalEncoder
from sklearn.utils import shuffle
from collections import Counter
import json

!pip install datasets

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("Zahra99/IEMOCAP_Text")

from datasets import concatenate_datasets

# Concatenate all sessions into a single dataset
concatenated_dataset = concatenate_datasets([
    dataset['session1'],
    dataset['session2'],
    dataset['session3'],
    dataset['session4'],
    dataset['session5']
])

print(concatenated_dataset)

def dataset(
    preloaded_dataset,  # The concatenated dataset that you've already loaded
    mode='dialog',
    embedding='random',
    embedding_size=50,
    primal_emotion='off',
    num_desired_classes=4,  # Update to reflect the number of classes
    return_dialogs=False
):
    dialogs_normalized = []
    dialogs = []
    words = []
    emotions_corpus = []

    # Step 1: Process the preloaded dataset
    for example in preloaded_dataset:
        dialog = []
        utterance = nltk.word_tokenize(example['text'].lower())  # Tokenize the sentence
        utterance_label = example['label']  # Use the label directly (since it's already an integer)

        dialog.append({
            'speaker': 'M',  # Assuming no speaker info, using default 'M'
            'sentence': utterance,
            'emotion': utterance_label
        })

        dialogs.append(dialog)
        dialogs_normalized.append(example['text'])
        words += utterance
        emotions_corpus.append(utterance_label)

    # Step 2: Create the word dictionary for the word2int transformation
    if embedding == 'random':
        words_count = Counter(words)
        words = set(word for word in words_count if words_count[word] >= 5)
        dictionary = {word: id + 1 for id, word in enumerate(words)}  # Create dictionary
        unknown = 0  # Set an ID for unknown words
        with open('dataset_dictionary.json', 'w') as fp:
            json.dump(dictionary, fp)

    elif embedding == 'glove':
        # Load GloVe embeddings
        filename = '../glove.6B.' + str(embedding_size) + 'd.txt'
        glove_vocab = loadGloVe(filename)
        dictionary = {glove_vocab[count]: count for count in range(len(glove_vocab))}
        unknown = dictionary.get('unk', 0)

    # Step 3: Transform emotions into integer IDs using OrdinalEncoder
    enc = OrdinalEncoder()
    sentences_targets = enc.fit_transform(np.array(emotions_corpus).reshape(-1, 1)).astype(int)
    print(enc.categories_)

    sentences_sources = []
    sentences_speakers = []
    sentences_lengths = []  # Store sentence lengths

    dialogs_sources_ = []
    dialogs_speakers_ = []
    dialogs_targets_ = []
    dialogs_lengths_ = []  # Store dialog-level sentence lengths

    # Step 4: Prepare the dataset (either dialog or sentence mode)
    for dialog in dialogs:
        utterances_sources = []
        utterances_speakers = []
        utterances_targets = []
        temp_sentences_length = []  # Keep track of sentence lengths for this dialog

        for utterance in dialog:
            sentence = [dictionary.get(word, unknown) for word in utterance['sentence']]
            utterances_sources.append(sentence)
            utterances_speakers.append([1, 0])  # Example speaker embedding, update as needed
            temp_sentences_length.append(len(sentence))
            utterances_targets.append(utterance['emotion'])

        dialogs_sources_.append(utterances_sources)
        dialogs_speakers_.append(utterances_speakers)
        dialogs_targets_.append(sentences_targets)  # Use sentences_targets instead of transforming again
        dialogs_lengths_.append(temp_sentences_length)  # Store sentence lengths for this dialog

        sentences_sources += utterances_sources
        sentences_speakers += utterances_speakers
        sentences_lengths += temp_sentences_length

    # Step 5: Split into train-validation-test sets
    total_dialogs = len(dialogs_sources_)
    train_idx = int(total_dialogs * 0.8)
    val_idx = int(total_dialogs * 0.9)

    train_list = list(range(train_idx))  # 80% train
    validation_list = list(range(train_idx, val_idx))  # 10% validation
    test_list = list(range(val_idx, total_dialogs))  # 10% test

    # Create train, validation, and test sets
    train_dialogs_sources = [dialogs_sources_[i] for i in train_list]
    train_sentences_sources = [sentence for dialog in train_dialogs_sources for sentence in dialog]

    validation_dialogs_sources = [dialogs_sources_[i] for i in validation_list]
    validation_sentences_sources = [sentence for dialog in validation_dialogs_sources for sentence in dialog]

    test_dialogs_sources = [dialogs_sources_[i] for i in test_list]
    test_sentences_sources = [sentence for dialog in test_dialogs_sources for sentence in dialog]

    # Prepare the sentences dataset information
    data_info = {
        'vocabulary_size': len(dictionary),
        'num_classes': len(enc.categories_[0]),
        'sentences_length': [len(sentence) for sentence in sentences_sources],  # Sentence lengths
        'dialogs_length': dialogs_lengths_,  # Dialog lengths (for dialog mode)
    }

    # Step 6: Return the datasets
    if return_dialogs:
        return dialogs_sources_, dialogs_targets_, data_info, dialogs_speakers_
    else:
        return sentences_sources, sentences_targets, data_info, sentences_speakers

def SentenceModel(vocab_size, embedding_size, first_rnn_size, num_classes, dropout, embedding, num_speakers):
    # Define input layers
    x = tf.keras.Input(shape=(None,), dtype=tf.int32)  # Input for sentences
    speaker = tf.keras.Input(shape=(2,), dtype=tf.int32)  # Input for speakers
    seqlen = tf.keras.Input(shape=(), dtype=tf.int32)  # Input for sequence length

    # Embedding layer
    if embedding == 'random':
        embeddings = tf.keras.layers.Embedding(vocab_size, embedding_size)(x)
    else:
        embeddings = tf.keras.layers.Embedding(vocab_size, embedding_size, trainable=False)(x)

    # Bidirectional LSTM
    lstm_fw = tf.keras.layers.LSTM(first_rnn_size, return_sequences=True)
    lstm_bw = tf.keras.layers.LSTM(first_rnn_size, return_sequences=True, go_backwards=True)
    rnn_outputs_fw = lstm_fw(embeddings)
    rnn_outputs_bw = lstm_bw(embeddings)

    rnn_outputs = tf.keras.layers.Concatenate(axis=-1)([rnn_outputs_fw, rnn_outputs_bw])

    # Gather the last relevant output according to sequence length
    def last_relevant_output(outputs, lengths):
        batch_size = tf.shape(outputs)[0]
        indices = tf.stack([tf.range(batch_size), lengths - 1], axis=1)
        return tf.gather_nd(outputs, indices)

    last_rnn_output = tf.keras.layers.Lambda(
        lambda x: last_relevant_output(x[0], x[1]),
        output_shape=(2 * first_rnn_size,)
    )([rnn_outputs, seqlen])

    if num_speakers:
        last_rnn_output = tf.keras.layers.Concatenate(axis=-1)([tf.cast(speaker, tf.float32), last_rnn_output])

    # Dropout layer
    dropout_layer = tf.keras.layers.Dropout(dropout)(last_rnn_output)

    # Output layer
    logits = tf.keras.layers.Dense(num_classes)(dropout_layer)
    preds = tf.keras.layers.Activation('softmax')(logits)

    # Create the model
    model = tf.keras.Model(inputs=[x, speaker, seqlen], outputs=preds)
    model.compile(optimizer=tf.keras.optimizers.Adam(5e-3),
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                  metrics=['accuracy'])

    return model

!pip install nltk
import nltk

# Download the 'punkt' data package
nltk.download('punkt')

# Now, the rest of your code...
from datasets import concatenate_datasets
sentences_sources, sentences_targets, data_info, sentences_speakers = dataset(
    preloaded_dataset=concatenated_dataset,
    mode='sentences',
    embedding='random',
    num_desired_classes=4  # Reflect the number of classes in your dataset
)

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Initialize the Sentence Model
batch_size = 32
vocab_size = data_info['vocabulary_size']  # Size of the vocabulary
embedding_size = 100  # Or any value based on your embeddings
first_rnn_size = 64   # Size of the LSTM hidden units for the first RNN
second_rnn_size = 64  # Size of the GRU hidden units for the second RNN
num_classes = 4       # Number of emotion classes (anger, happy, neutral, sad)
dropout = 0.5         # Dropout rate for regularization
score_type = 'dot'    # Scoring function for attention (dot, general, concat)
model_type = 'RNN_with_memory'  # Model type
window_size = -1      # Use the entire dialog for attention
embedding = 'random'  # 'random' for random embeddings or 'glove' for GloVe embeddings
num_speakers = False  # False if you're not using speaker information

sentences, targets, data_info, speakers = dataset(preloaded_dataset=concatenated_dataset, mode='dialog', embedding=embedding, embedding_size=embedding_size)
sentences_length = [len(sentence) for sentence in sentences]

model1 = SentenceModel(vocab_size, embedding_size, first_rnn_size, num_classes, dropout, embedding, num_speakers)

!pip install sklearn seaborn matplotlib

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Concatenate, Dropout, Bidirectional
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have your 'sentences', 'speakers', 'sentences_length', and 'targets' ready
# Define batch size and number of epochs
batch_size = 32
epochs = 5

# Let's assume 'sentences' is already tokenized and is a list of lists of integers
vocab_size = max([max(sentence) for sentence in sentences if len(sentence) > 0]) + 1
max_length = max([len(sentence) for sentence in sentences])  # Maximum length of sentences

# Stratified split of the data into train, validation, and test sets
sentences_padded = pad_sequences(sentences, maxlen=max_length, padding='post')
speakers_np = np.array(speakers)  # Assuming speakers are in proper shape
sentences_length_np = np.array(sentences_length)  # Assuming it's correctly shaped
targets_np = np.array(targets)  # Targets should be an array of integer labels

# Split the data with stratified splitting to maintain class distribution across splits
sentences_train, sentences_test, speakers_train, speakers_test, lengths_train, lengths_test, targets_train, targets_test = train_test_split(
    sentences_padded, speakers_np, sentences_length_np, targets_np, test_size=0.2, stratify=targets_np
)

# Further split the training data into training and validation sets
sentences_train, sentences_val, speakers_train, speakers_val, lengths_train, lengths_val, targets_train, targets_val = train_test_split(
    sentences_train, speakers_train, lengths_train, targets_train, test_size=0.25, stratify=targets_train
)

# Flatten the target arrays to make them compatible with compute_class_weight
targets_train_flat = targets_train.flatten()

# Compute class weights to handle class imbalance
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(targets_train_flat),
    y=targets_train_flat
)
class_weights_dict = dict(enumerate(class_weights))

# Define the model
def build_model(vocab_size, embedding_dim=128, lstm_units=64, dense_units=64, num_classes=4):
    # Sentence input
    input_sentences = Input(shape=(None,), dtype='int32')

    # Speaker input
    input_speakers = Input(shape=(2,), dtype='float32')

    # Sentence length input
    input_sentence_lengths = Input(shape=(1,), dtype='int32')

    # Embedding Layer for sentences
    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)
    embedded_sentences = embedding_layer(input_sentences)

    # Apply Bidirectional LSTM to embedded sentences
    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=False))(embedded_sentences)

    # Concatenate speaker information (if using speaker info)
    concatenated = Concatenate()([lstm_out, input_speakers])

    # Fully connected layer after concatenation
    dense_out = Dense(dense_units, activation='relu')(concatenated)

    # Dropout layer for regularization
    dropout_out = Dropout(0.5)(dense_out)

    # Output layer (softmax for classification)
    output = Dense(num_classes, activation='softmax')(dropout_out)

    # Create the model
    model = Model(inputs=[input_sentences, input_speakers, input_sentence_lengths], outputs=output)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

# Build the model with the computed vocab size
model1 = build_model(vocab_size=vocab_size)

# Train the model with class weights to handle the imbalance
history = model1.fit(
    [sentences_train, speakers_train, lengths_train],
    targets_train,
    validation_data=([sentences_val, speakers_val, lengths_val], targets_val),
    batch_size=batch_size,
    epochs=epochs,
    class_weight=class_weights_dict  # Use class weights
)

# Evaluate the model after training
test_loss, test_accuracy = model1.evaluate([sentences_test, speakers_test, lengths_test], targets_test)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# Predict on test data
predictions = model1.predict([sentences_test, speakers_test, lengths_test])
predicted_classes = np.argmax(predictions, axis=1)

# Generate the classification report
print("Classification Report:")
print(classification_report(targets_test, predicted_classes, target_names=['angry', 'happy', 'neutral', 'sad']))

# Compute confusion matrix
cm = confusion_matrix(targets_test, predicted_classes)
print("Confusion Matrix:")
print(cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['angry', 'happy', 'neutral', 'sad'], yticklabels=['angry', 'happy', 'neutral', 'sad'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Compute class-wise precision, recall, and F1-score
precision = np.diag(cm) / np.sum(cm, axis=0)
recall = np.diag(cm) / np.sum(cm, axis=1)
f1_scores = 2 * precision * recall / (precision + recall)

print("Class-wise precision:", precision)
print("Class-wise recall:", recall)
print("Class-wise F1-score:", f1_scores)

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
import nltk
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

nltk.download('punkt')

# 1. Function to create sliding windows of past 5 sentences
def create_contextual_windows(dataset, window_size=10):
    contexts = []
    targets = []
    current_context = []

    for i, example in enumerate(dataset):
        sentence = example['text']
        emotion = example['label']
        current_context.append(sentence)

        if len(current_context) > window_size:
            current_context.pop(0)

        if len(current_context) >= 1:
            contexts.append(list(current_context))
            targets.append(emotion)

    return contexts, targets

# 2. Function to split data into train, val, test sets
def stratified_split_data(dialogs, labels, train_size=0.8, val_size=0.1):
    from sklearn.model_selection import train_test_split
    train_dialogs, temp_dialogs, train_labels, temp_labels = train_test_split(
        dialogs, labels, train_size=train_size, stratify=labels, random_state=42
    )

    val_size_adjusted = val_size / (1 - train_size)
    val_dialogs, test_dialogs, val_labels, test_labels = train_test_split(
        temp_dialogs, temp_labels, train_size=val_size_adjusted, stratify=temp_labels, random_state=42
    )

    return train_dialogs, val_dialogs, test_dialogs, train_labels, val_labels, test_labels

# 3. Initialize tokenizer
tokenizer = Tokenizer(num_words=5000)
all_texts = [example['text'] for example in concatenated_dataset]
tokenizer.fit_on_texts(all_texts)

# 4. Tokenize and pad dialogs
def tokenize_dialogs(dialogs):
    return [tokenizer.texts_to_sequences(dialog) for dialog in dialogs]

def pad_dialogs(dialogs, max_dialog_len, max_sentence_len):
    tokenized_dialogs = tokenize_dialogs(dialogs)
    return pad_sequences(
        [pad_sequences(dialog, maxlen=max_sentence_len, padding='post') for dialog in tokenized_dialogs],
        maxlen=max_dialog_len, padding='post'
    )

# 5. Define model with attention-based pooling and multi-head attention
def build_dialog_model(vocab_size, embedding_size, sentence_rnn_size, dialog_rnn_size, num_classes, dropout_rate):
    input_dialog = tf.keras.Input(shape=(None, None), dtype=tf.int32)

    embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size)
    embedded_sentences = tf.keras.layers.TimeDistributed(embedding_layer)(input_dialog)

    sentence_lstm_fw = tf.keras.layers.LSTM(sentence_rnn_size // 2, return_sequences=True)
    sentence_lstm_bw = tf.keras.layers.LSTM(sentence_rnn_size // 2, return_sequences=True, go_backwards=True)
    sentence_rnn = tf.keras.layers.TimeDistributed(
        tf.keras.layers.Bidirectional(sentence_lstm_fw)
    )(embedded_sentences)

    sentence_rnn_pooled = tf.keras.layers.TimeDistributed(
        tf.keras.layers.GlobalAveragePooling1D()
    )(sentence_rnn)

    dialog_gru = tf.keras.layers.GRU(dialog_rnn_size, return_sequences=True)(sentence_rnn_pooled)

    # Multi-Head Attention
    multihead_attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=dialog_rnn_size)(dialog_gru, dialog_gru)

    # Attention Pooling
    attention_weights = tf.keras.layers.Dense(1, activation="tanh")(multihead_attention)
    attention_weights = tf.keras.layers.Flatten()(attention_weights)
    attention_weights = tf.keras.layers.Activation("softmax")(attention_weights)
    attention_weights = tf.keras.layers.RepeatVector(dialog_rnn_size)(attention_weights)
    attention_weights = tf.keras.layers.Permute([2, 1])(attention_weights)

    dialog_representation = tf.keras.layers.multiply([multihead_attention, attention_weights])
    dialog_representation = tf.keras.layers.GlobalAveragePooling1D()(dialog_representation)

    output = tf.keras.layers.Dense(num_classes, activation='softmax')(dialog_representation)

    model = tf.keras.Model(inputs=input_dialog, outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 6. Set parameters
vocab_size = 5000
embedding_size = 100
sentence_rnn_size = 128
dialog_rnn_size = 128
num_classes = 4
dropout_rate = 0.5
batch_size = 32
epochs = 10
max_dialog_len = 10
max_sentence_len = 100

# 7. Create contextual dialogs and split data
dialogs_contexts, dialogs_targets = create_contextual_windows(concatenated_dataset, window_size=5)
train_dialogs, val_dialogs, test_dialogs, train_labels, val_labels, test_labels = stratified_split_data(dialogs_contexts, dialogs_targets)

# 8. Pad dialogs
train_dialogs_padded = pad_dialogs(train_dialogs, max_dialog_len, max_sentence_len)
val_dialogs_padded = pad_dialogs(val_dialogs, max_dialog_len, max_sentence_len)
test_dialogs_padded = pad_dialogs(test_dialogs, max_dialog_len, max_sentence_len)

# 9. Build and train model
model = build_dialog_model(vocab_size, embedding_size, sentence_rnn_size, dialog_rnn_size, num_classes, dropout_rate)
history = model.fit(
    train_dialogs_padded, np.array(train_labels),
    validation_data=(val_dialogs_padded, np.array(val_labels)),
    batch_size=batch_size,
    epochs=epochs
)

# 10. Evaluate and generate report
test_loss, test_accuracy = model.evaluate(test_dialogs_padded, np.array(test_labels), verbose=0)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

y_pred = model.predict(test_dialogs_padded)
y_pred_labels = np.argmax(y_pred, axis=1)

conf_matrix = confusion_matrix(test_labels, y_pred_labels)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['angry', 'happy', 'neutral', 'sad'], yticklabels=['angry', 'happy', 'neutral', 'sad'])
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()

report = classification_report(test_labels, y_pred_labels, target_names=['angry', 'happy', 'neutral', 'sad'])
print("\nClassification Report:")
print(report)

# import tensorflow as tf
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.preprocessing.text import Tokenizer
# import numpy as np
# import nltk
# from sklearn.metrics import confusion_matrix, classification_report
# import seaborn as sns
# import matplotlib.pyplot as plt

# nltk.download('punkt')

# # 1. Function to create sliding windows of past 5 sentences
# def create_contextual_windows(dataset, window_size=5):
#     contexts = []
#     targets = []
#     current_context = []

#     # Iterate through each sentence in the dataset
#     for i, example in enumerate(dataset):
#         sentence = example['text']
#         emotion = example['label']

#         # Add current sentence to context
#         current_context.append(sentence)

#         # If context has more than the window size, remove the oldest sentence
#         if len(current_context) > window_size:
#             current_context.pop(0)

#         # If we have enough sentences (at least one), add context and target
#         if len(current_context) >= 1:
#             contexts.append(list(current_context))  # Copy of the current context
#             targets.append(emotion)

#     return contexts, targets

# # 2. Function to split data into train, val, test sets
# def split_data(dialogs, labels, train_size=0.8, val_size=0.1):
#     total_size = len(dialogs)
#     train_idx = int(total_size * train_size)
#     val_idx = int(total_size * (train_size + val_size))

#     train_dialogs = dialogs[:train_idx]
#     val_dialogs = dialogs[train_idx:val_idx]
#     test_dialogs = dialogs[val_idx:]

#     train_labels = labels[:train_idx]
#     val_labels = labels[train_idx:val_idx]
#     test_labels = labels[val_idx:]

#     return train_dialogs, val_dialogs, test_dialogs, train_labels, val_labels, test_labels

# # 3. Initialize the tokenizer
# tokenizer = Tokenizer(num_words=5000)  # Adjust vocab size as needed

# # 4. Tokenize and fit on the full dataset
# all_texts = [example['text'] for example in concatenated_dataset]
# tokenizer.fit_on_texts(all_texts)

# # 5. Convert dialogs to sequences of integer tokens using the tokenizer
# def tokenize_dialogs(dialogs):
#     return [tokenizer.texts_to_sequences(dialog) for dialog in dialogs]

# # 6. Pad sequences function
# def pad_dialogs(dialogs, max_dialog_len, max_sentence_len):
#     tokenized_dialogs = tokenize_dialogs(dialogs)
#     return pad_sequences(
#         [pad_sequences(dialog, maxlen=max_sentence_len, padding='post') for dialog in tokenized_dialogs],
#         maxlen=max_dialog_len, padding='post'
#     )

# # 7. Define the model (with TimeDistributed to handle 4D input)
# def build_dialog_model(vocab_size, embedding_size, sentence_rnn_size, dialog_rnn_size, num_classes, dropout_rate):
#     # Define inputs
#     input_dialog = tf.keras.Input(shape=(None, None), dtype=tf.int32)  # Shape: (batch_size, num_sentences, num_words)

#     # Word-level Embedding
#     embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size)
#     embedded_sentences = tf.keras.layers.TimeDistributed(embedding_layer)(input_dialog)

#     # Sentence-level BiLSTM with TimeDistributed
#     sentence_lstm_fw = tf.keras.layers.LSTM(sentence_rnn_size // 2, return_sequences=True)
#     sentence_lstm_bw = tf.keras.layers.LSTM(sentence_rnn_size // 2, return_sequences=True, go_backwards=True)

#     # Apply Bidirectional LSTM to each sentence in the dialog
#     sentence_rnn = tf.keras.layers.TimeDistributed(
#         tf.keras.layers.Bidirectional(sentence_lstm_fw)
#     )(embedded_sentences)

#     # Apply GlobalAveragePooling1D to reduce each sentence's word-level representations to a single vector
#     sentence_rnn_pooled = tf.keras.layers.TimeDistributed(
#         tf.keras.layers.GlobalAveragePooling1D()
#     )(sentence_rnn)

#     # Dialog-level GRU to process the sequence of sentences in a dialog
#     dialog_gru = tf.keras.layers.GRU(dialog_rnn_size, return_sequences=True)(sentence_rnn_pooled)

#     # Self-Attention Layer (Optional)
#     attention = tf.keras.layers.Attention()([dialog_gru, dialog_gru])  # Self-attention over dialog

#     # GlobalAveragePooling1D instead of tf.reduce_mean for handling varying sequence lengths
#     dialog_representation = tf.keras.layers.GlobalAveragePooling1D()(attention)  # (batch_size, rnn_units)

#     # Output Layer
#     output = tf.keras.layers.Dense(num_classes, activation='softmax')(dialog_representation)

#     # Create the model
#     model = tf.keras.Model(inputs=input_dialog, outputs=output)
#     model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
#                   loss='sparse_categorical_crossentropy',
#                   metrics=['accuracy'])
#     return model

# # 8. Set parameters
# vocab_size = 5000  # Example, update based on your tokenizer
# embedding_size = 100
# sentence_rnn_size = 128
# dialog_rnn_size = 128
# num_classes = 4  # Number of emotion classes
# dropout_rate = 0.5
# batch_size = 32
# epochs = 7
# max_dialog_len = 10
# max_sentence_len = 100  # Assuming 100 words max per sentence

# # 9. Create contextual dialogs (sliding window approach)
# from sklearn.model_selection import train_test_split

# # 2. Function to split data with stratified sampling
# def stratified_split_data(dialogs, labels, train_size=0.8, val_size=0.1):
#     # First, split into train and temp (which will later be split into validation and test)
#     train_dialogs, temp_dialogs, train_labels, temp_labels = train_test_split(
#         dialogs, labels, train_size=train_size, stratify=labels, random_state=42
#     )

#     # Split temp into validation and test sets
#     val_size_adjusted = val_size / (1 - train_size)  # Adjust validation size relative to the temp set
#     val_dialogs, test_dialogs, val_labels, test_labels = train_test_split(
#         temp_dialogs, temp_labels, train_size=val_size_adjusted, stratify=temp_labels, random_state=42
#     )

#     return train_dialogs, val_dialogs, test_dialogs, train_labels, val_labels, test_labels

# # 9. Create contextual dialogs (sliding window approach)
# dialogs_contexts, dialogs_targets = create_contextual_windows(concatenated_dataset, window_size=5)

# # 10. Split the data using stratified sampling
# train_dialogs, val_dialogs, test_dialogs, train_labels, val_labels, test_labels = stratified_split_data(dialogs_contexts, dialogs_targets)

# # 11. Pad the dialogs
# train_dialogs_padded = pad_dialogs(train_dialogs, max_dialog_len, max_sentence_len)
# val_dialogs_padded = pad_dialogs(val_dialogs, max_dialog_len, max_sentence_len)
# test_dialogs_padded = pad_dialogs(test_dialogs, max_dialog_len, max_sentence_len)

# # 12. Build and train the model as before
# model = build_dialog_model(vocab_size, embedding_size, sentence_rnn_size, dialog_rnn_size, num_classes, dropout_rate)
# history = model.fit(
#     train_dialogs_padded, np.array(train_labels),
#     validation_data=(val_dialogs_padded, np.array(val_labels)),
#     batch_size=batch_size,
#     epochs=epochs
# )


# # 14. Evaluate the model on the test set
# test_loss, test_accuracy = model.evaluate(test_dialogs_padded, np.array(test_labels), verbose=0)
# print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# # 15. Predict on the test set
# y_pred = model.predict(test_dialogs_padded)
# y_pred_labels = np.argmax(y_pred, axis=1)

# # 16. Confusion Matrix
# conf_matrix = confusion_matrix(test_labels, y_pred_labels)
# print("Confusion Matrix:")
# print(conf_matrix)

# # Plot the confusion matrix
# plt.figure(figsize=(8, 6))
# sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['angry', 'happy', 'neutral', 'sad'], yticklabels=['angry', 'happy', 'neutral', 'sad'])
# plt.ylabel('True label')
# plt.xlabel('Predicted label')
# plt.title('Confusion Matrix')
# plt.show()

# # 17. Classification Report
# report = classification_report(test_labels, y_pred_labels, target_names=['angry', 'happy', 'neutral', 'sad'])
# print("\nClassification Report:")
# print(report)

